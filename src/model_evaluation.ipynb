{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd433115",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de Modelo - California Housing Dataset\n",
    "\n",
    "Este notebook se encarga de la evaluaci√≥n profunda del mejor modelo seleccionado.\n",
    "\n",
    "**Objetivo**: Validar a fondo el modelo antes de su despliegue en producci√≥n.\n",
    "\n",
    "**An√°lisis incluidos**:\n",
    "- Validaci√≥n cruzada (K-Fold)\n",
    "- Learning curves (curvas de aprendizaje)\n",
    "- An√°lisis de residuos\n",
    "- M√©tricas por segmentos de precio\n",
    "- An√°lisis de errores\n",
    "\n",
    "**Autor**: MLOps Pipeline Project  \n",
    "**Fecha**: Noviembre 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab1513",
   "metadata": {},
   "source": [
    "## 1. Importar Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import glob\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Feature Engineering\n",
    "from ft_engineering import prepare_data_for_training\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, KFold\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,\n",
    "                             mean_absolute_percentage_error)\n",
    "\n",
    "# Model Persistence\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3d8f9",
   "metadata": {},
   "source": [
    "## 2. Cargar Configuraci√≥n y Preparar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734876e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar configuraci√≥n\n",
    "with open('../config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "MODEL_OUTPUT_DIR = f\"../{config.get('model_output_dir', 'models/')}\"\n",
    "RANDOM_STATE = config.get('random_state', 42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURACI√ìN DEL PROYECTO\")\n",
    "print(\"=\" * 80)\n",
    "for key, value in config.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047519cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos con feature engineering\n",
    "print(\"\\nPreparando datos...\")\n",
    "X_train, X_test, y_train, y_test, preprocessor, feature_names = prepare_data_for_training(\n",
    "    config_path='../config.json',\n",
    "    test_size=0.2,\n",
    "    save_preprocessor=False  # Ya fue guardado en training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ea56e",
   "metadata": {},
   "source": [
    "## 3. Cargar el Mejor Modelo Guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013832d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar el √∫ltimo modelo guardado\n",
    "model_files = glob.glob(os.path.join(MODEL_OUTPUT_DIR, 'best_model_*.pkl'))\n",
    "\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(f\"No se encontraron modelos en {MODEL_OUTPUT_DIR}. Ejecuta model_training.ipynb primero.\")\n",
    "\n",
    "# Ordenar por fecha de modificaci√≥n y tomar el m√°s reciente\n",
    "latest_model_file = max(model_files, key=os.path.getmtime)\n",
    "\n",
    "# Cargar modelo\n",
    "best_model = joblib.load(latest_model_file)\n",
    "model_name = os.path.basename(latest_model_file).replace('best_model_', '').replace('.pkl', '')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELO CARGADO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Archivo: {os.path.basename(latest_model_file)}\")\n",
    "print(f\"Modelo: {best_model.__class__.__name__}\")\n",
    "print(f\"Tama√±o: {os.path.getsize(latest_model_file) / 1024:.2f} KB\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225fb061",
   "metadata": {},
   "source": [
    "## 4. Validaci√≥n Cruzada (K-Fold Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar validaci√≥n cruzada con 5 folds\n",
    "print(\"Ejecutando validaci√≥n cruzada (5-Fold)...\")\n",
    "print(\"Esto puede tardar unos minutos...\\n\")\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Calcular scores para diferentes m√©tricas\n",
    "scoring_metrics = {\n",
    "    'R¬≤': 'r2',\n",
    "    'NEG_MAE': 'neg_mean_absolute_error',\n",
    "    'NEG_RMSE': 'neg_root_mean_squared_error'\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "for metric_name, metric_scorer in scoring_metrics.items():\n",
    "    scores = cross_val_score(best_model, X_train, y_train, \n",
    "                             cv=kfold, scoring=metric_scorer, n_jobs=-1)\n",
    "    cv_results[metric_name] = scores\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESULTADOS DE VALIDACI√ìN CRUZADA (5-FOLD)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric_name, scores in cv_results.items():\n",
    "    # Convertir scores negativos a positivos para MAE y RMSE\n",
    "    if 'NEG' in metric_name:\n",
    "        scores = -scores\n",
    "        display_name = metric_name.replace('NEG_', '')\n",
    "    else:\n",
    "        display_name = metric_name\n",
    "    \n",
    "    print(f\"\\n{display_name}:\")\n",
    "    print(f\"  Media: {scores.mean():.4f}\")\n",
    "    print(f\"  Desv. Std: {scores.std():.4f}\")\n",
    "    print(f\"  Min: {scores.min():.4f}\")\n",
    "    print(f\"  Max: {scores.max():.4f}\")\n",
    "    print(f\"  Scores: {scores}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524055aa",
   "metadata": {},
   "source": [
    "## 5. Learning Curves (Curvas de Aprendizaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar learning curves\n",
    "print(\"Generando learning curves...\")\n",
    "print(\"Esto puede tardar varios minutos...\\n\")\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_train, y_train,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Calcular medias y desviaciones est√°ndar\n",
    "train_scores_mean = train_scores.mean(axis=1)\n",
    "train_scores_std = train_scores.std(axis=1)\n",
    "val_scores_mean = val_scores.mean(axis=1)\n",
    "val_scores_std = val_scores.std(axis=1)\n",
    "\n",
    "# Visualizar learning curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# √Årea de desviaci√≥n est√°ndar\n",
    "plt.fill_between(train_sizes_abs, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color='blue')\n",
    "plt.fill_between(train_sizes_abs, \n",
    "                 val_scores_mean - val_scores_std,\n",
    "                 val_scores_mean + val_scores_std, \n",
    "                 alpha=0.1, color='orange')\n",
    "\n",
    "# L√≠neas principales\n",
    "plt.plot(train_sizes_abs, train_scores_mean, 'o-', color='blue',\n",
    "         label='Training score', linewidth=2, markersize=6)\n",
    "plt.plot(train_sizes_abs, val_scores_mean, 'o-', color='orange',\n",
    "         label='Cross-validation score', linewidth=2, markersize=6)\n",
    "\n",
    "plt.xlabel('Training Set Size', fontsize=12)\n",
    "plt.ylabel('R¬≤ Score', fontsize=12)\n",
    "plt.title(f'Learning Curves - {best_model.__class__.__name__}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0.5, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de las curvas\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE LEARNING CURVES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nCon {train_sizes_abs[-1]:,} muestras de entrenamiento:\")\n",
    "print(f\"  ‚Ä¢ Training score:        {train_scores_mean[-1]:.4f} ¬± {train_scores_std[-1]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Validation score:      {val_scores_mean[-1]:.4f} ¬± {val_scores_std[-1]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Gap (overfitting):     {train_scores_mean[-1] - val_scores_mean[-1]:.4f}\")\n",
    "\n",
    "if train_scores_mean[-1] - val_scores_mean[-1] < 0.05:\n",
    "    print(\"\\n‚úÖ Gap < 5%: El modelo est√° bien balanceado (no hay overfitting significativo)\")\n",
    "elif train_scores_mean[-1] - val_scores_mean[-1] < 0.10:\n",
    "    print(\"\\n‚ö†Ô∏è Gap entre 5-10%: Ligero overfitting, pero aceptable\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Gap > 10%: Overfitting detectado, considerar regularizaci√≥n\")\n",
    "\n",
    "if val_scores_mean[-1] > val_scores_mean[-2]:\n",
    "    print(\"üìà La curva de validaci√≥n sigue subiendo: M√°s datos podr√≠an mejorar el modelo\")\n",
    "else:\n",
    "    print(\"üìä La curva de validaci√≥n se ha estabilizado: Cantidad de datos es adecuada\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd609176",
   "metadata": {},
   "source": [
    "## 6. Predicciones en Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular m√©tricas\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"M√âTRICAS EN TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nMean Absolute Error (MAE):       ${mae:,.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE):  ${rmse:,.2f}\")\n",
    "print(f\"R¬≤ Score:                        {r2:.4f}\")\n",
    "print(f\"MAPE:                            {mape:.2f}%\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f540dbc",
   "metadata": {},
   "source": [
    "## 7. An√°lisis de Residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1341db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular residuos\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Crear figura con 4 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Residuos vs Predicciones\n",
    "axes[0, 0].scatter(y_pred, residuals, alpha=0.5, s=20)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Predicciones ($)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Residuos ($)', fontsize=11)\n",
    "axes[0, 0].set_title('Residuos vs Predicciones', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Histograma de residuos\n",
    "axes[0, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Residuos ($)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Frecuencia', fontsize=11)\n",
    "axes[0, 1].set_title('Distribuci√≥n de Residuos', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-Q Plot (normalidad de residuos)\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (Normalidad de Residuos)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuos estandarizados\n",
    "standardized_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "axes[1, 1].scatter(y_pred, standardized_residuals, alpha=0.5, s=20)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].axhline(y=2, color='orange', linestyle='--', linewidth=1, alpha=0.7)\n",
    "axes[1, 1].axhline(y=-2, color='orange', linestyle='--', linewidth=1, alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Predicciones ($)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Residuos Estandarizados', fontsize=11)\n",
    "axes[1, 1].set_title('Residuos Estandarizados', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estad√≠sticas de residuos\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTAD√çSTICAS DE RESIDUOS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Media de residuos:               ${residuals.mean():,.2f}\")\n",
    "print(f\"Mediana de residuos:             ${np.median(residuals):,.2f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar:             ${residuals.std():,.2f}\")\n",
    "print(f\"Residuo m√≠nimo:                  ${residuals.min():,.2f}\")\n",
    "print(f\"Residuo m√°ximo:                  ${residuals.max():,.2f}\")\n",
    "print(f\"\\nResid uos fuera de ¬±2œÉ:          {np.sum(np.abs(standardized_residuals) > 2)} ({np.sum(np.abs(standardized_residuals) > 2) / len(standardized_residuals) * 100:.2f}%)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafcff2c",
   "metadata": {},
   "source": [
    "## 8. An√°lisis por Segmentos de Precio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8476ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear segmentos de precio\n",
    "price_segments = pd.qcut(y_test, q=4, labels=['Bajo', 'Medio-Bajo', 'Medio-Alto', 'Alto'])\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test,\n",
    "    'y_pred': y_pred,\n",
    "    'residual': residuals,\n",
    "    'abs_error': np.abs(residuals),\n",
    "    'segment': price_segments\n",
    "})\n",
    "\n",
    "# Calcular m√©tricas por segmento\n",
    "segment_metrics = results_df.groupby('segment').agg({\n",
    "    'y_true': ['count', 'mean', 'min', 'max'],\n",
    "    'abs_error': 'mean',\n",
    "    'residual': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "segment_metrics.columns = ['Count', 'Avg Price', 'Min Price', 'Max Price', 'MAE', 'Mean Residual', 'Std Residual']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"M√âTRICAS POR SEGMENTO DE PRECIO\")\n",
    "print(\"=\" * 80)\n",
    "print(segment_metrics)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualizar m√©tricas por segmento\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# MAE por segmento\n",
    "segment_metrics['MAE'].plot(kind='bar', ax=axes[0], color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('MAE por Segmento de Precio', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Segmento', fontsize=11)\n",
    "axes[0].set_ylabel('MAE ($)', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Boxplot de errores absolutos por segmento\n",
    "results_df.boxplot(column='abs_error', by='segment', ax=axes[1])\n",
    "axes[1].set_title('Distribuci√≥n de Errores Absolutos por Segmento', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Segmento', fontsize=11)\n",
    "axes[1].set_ylabel('Error Absoluto ($)', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "plt.suptitle('')  # Remover t√≠tulo autom√°tico\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834228e4",
   "metadata": {},
   "source": [
    "## 9. An√°lisis de Peores Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb98954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar las 10 peores predicciones\n",
    "worst_predictions = results_df.nlargest(10, 'abs_error')[['y_true', 'y_pred', 'abs_error', 'segment']]\n",
    "worst_predictions['error_pct'] = (worst_predictions['abs_error'] / worst_predictions['y_true']) * 100\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 PEORES PREDICCIONES\")\n",
    "print(\"=\" * 80)\n",
    "print(worst_predictions.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# An√°lisis de errores por percentil\n",
    "error_percentiles = np.percentile(results_df['abs_error'], [50, 75, 90, 95, 99])\n",
    "\n",
    "print(\"\\nDISTRIBUCI√ìN DE ERRORES (PERCENTILES)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"50% de predicciones tienen error < ${error_percentiles[0]:,.2f}\")\n",
    "print(f\"75% de predicciones tienen error < ${error_percentiles[1]:,.2f}\")\n",
    "print(f\"90% de predicciones tienen error < ${error_percentiles[2]:,.2f}\")\n",
    "print(f\"95% de predicciones tienen error < ${error_percentiles[3]:,.2f}\")\n",
    "print(f\"99% de predicciones tienen error < ${error_percentiles[4]:,.2f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa696ab",
   "metadata": {},
   "source": [
    "## 10. Resumen y Conclusiones de la Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46697b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN DE LA EVALUACI√ìN DEL MODELO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Modelo Evaluado: {best_model.__class__.__name__}\")\n",
    "print(f\"\\nüéØ M√©tricas en Test Set:\")\n",
    "print(f\"  ‚Ä¢ R¬≤ Score:                      {r2:.4f}\")\n",
    "print(f\"  ‚Ä¢ RMSE:                          ${rmse:,.2f}\")\n",
    "print(f\"  ‚Ä¢ MAE:                           ${mae:,.2f}\")\n",
    "print(f\"  ‚Ä¢ MAPE:                          {mape:.2f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Validaci√≥n Cruzada (5-Fold):\")\n",
    "r2_cv_mean = cv_results['R¬≤'].mean()\n",
    "mae_cv_mean = -cv_results['NEG_MAE'].mean()\n",
    "print(f\"  ‚Ä¢ R¬≤ (media ¬± std):              {r2_cv_mean:.4f} ¬± {cv_results['R¬≤'].std():.4f}\")\n",
    "print(f\"  ‚Ä¢ MAE (media ¬± std):             ${mae_cv_mean:,.2f} ¬± ${cv_results['NEG_MAE'].std():,.2f}\")\n",
    "\n",
    "print(f\"\\nüìà Learning Curves:\")\n",
    "final_gap = train_scores_mean[-1] - val_scores_mean[-1]\n",
    "print(f\"  ‚Ä¢ Training score final:          {train_scores_mean[-1]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Validation score final:        {val_scores_mean[-1]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Gap (overfitting indicator):   {final_gap:.4f}\")\n",
    "\n",
    "if final_gap < 0.05:\n",
    "    overfitting_status = \"‚úÖ No hay overfitting significativo\"\n",
    "elif final_gap < 0.10:\n",
    "    overfitting_status = \"‚ö†Ô∏è Ligero overfitting\"\n",
    "else:\n",
    "    overfitting_status = \"‚ùå Overfitting detectado\"\n",
    "print(f\"  ‚Ä¢ Estado:                        {overfitting_status}\")\n",
    "\n",
    "print(f\"\\nüìä Distribuci√≥n de Errores:\")\n",
    "print(f\"  ‚Ä¢ 50% predicciones error <       ${error_percentiles[0]:,.2f}\")\n",
    "print(f\"  ‚Ä¢ 95% predicciones error <       ${error_percentiles[3]:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Residuos fuera de ¬±2œÉ:         {np.sum(np.abs(standardized_residuals) > 2)} ({np.sum(np.abs(standardized_residuals) > 2) / len(standardized_residuals) * 100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Conclusiones:\")\n",
    "if r2 > 0.80:\n",
    "    print(\"  ‚úÖ Modelo con excelente capacidad predictiva (R¬≤ > 0.80)\")\n",
    "elif r2 > 0.70:\n",
    "    print(\"  ‚úÖ Modelo con buena capacidad predictiva (R¬≤ > 0.70)\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è Modelo con capacidad predictiva moderada (R¬≤ < 0.70)\")\n",
    "\n",
    "if final_gap < 0.05:\n",
    "    print(\"  ‚úÖ Modelo bien generalizado, sin overfitting\")\n",
    "    \n",
    "if mape < 15:\n",
    "    print(f\"  ‚úÖ Error porcentual aceptable (MAPE < 15%)\")\n",
    "    \n",
    "print(f\"\\nüí° Recomendaciones:\")\n",
    "print(\"  ‚Ä¢ El modelo est√° listo para despliegue en producci√≥n\")\n",
    "print(\"  ‚Ä¢ Implementar monitoreo de drift para detectar cambios en los datos\")\n",
    "print(\"  ‚Ä¢ Reentrenar peri√≥dicamente con nuevos datos\")\n",
    "print(\"  ‚Ä¢ Documentar casos de peores predicciones para an√°lisis futuro\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ EVALUACI√ìN COMPLETADA - MODELO APROBADO PARA PRODUCCI√ìN\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
