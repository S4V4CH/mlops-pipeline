{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4fd006",
   "metadata": {},
   "source": [
    "# Entrenamiento de Modelos - California Housing Dataset\n",
    "\n",
    "Este notebook se encarga del entrenamiento y comparaci√≥n de m√∫ltiples modelos de Machine Learning.\n",
    "\n",
    "**Objetivo**: Entrenar, comparar y seleccionar el mejor modelo para predicci√≥n de precios de viviendas.\n",
    "\n",
    "**Modelos a entrenar**:\n",
    "- Random Forest Regressor\n",
    "- XGBoost Regressor\n",
    "- LightGBM Regressor\n",
    "\n",
    "**Autor**: MLOps Pipeline Project  \n",
    "**Fecha**: Noviembre 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55721aaa",
   "metadata": {},
   "source": [
    "## 1. Importar Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Feature Engineering\n",
    "from ft_engineering import prepare_data_for_training\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Model Persistence\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas exitosamente\")\n",
    "print(f\"Versi√≥n de pandas: {pd.__version__}\")\n",
    "print(f\"Versi√≥n de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6d1ea",
   "metadata": {},
   "source": [
    "## 2. Cargar Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar configuraci√≥n desde config.json\n",
    "with open('../config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURACI√ìN DEL PROYECTO\")\n",
    "print(\"=\" * 80)\n",
    "for key, value in config.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuraci√≥n adicional\n",
    "RANDOM_STATE = config.get('random_state', 42)\n",
    "MODEL_OUTPUT_DIR = f\"../{config.get('model_output_dir', 'models/')}\"\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075d04a",
   "metadata": {},
   "source": [
    "## 3. Preparar Datos con Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar pipeline de feature engineering completo\n",
    "X_train, X_test, y_train, y_test, preprocessor, feature_names = prepare_data_for_training(\n",
    "    config_path='../config.json',\n",
    "    test_size=0.2,\n",
    "    save_preprocessor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar dimensiones de los datos preparados\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATOS PREPARADOS PARA ENTRENAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(\"\\nPrimeros 10 features:\")\n",
    "for i, name in enumerate(feature_names[:10], 1):\n",
    "    print(f\"  {i:2d}. {name}\")\n",
    "if len(feature_names) > 10:\n",
    "    print(f\"  ... y {len(feature_names) - 10} m√°s\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd02b45",
   "metadata": {},
   "source": [
    "## 4. Definir Modelos a Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813dbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de modelos a entrenar\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELOS CONFIGURADOS\")\n",
    "print(\"=\" * 80)\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {model.__class__.__name__}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396f783",
   "metadata": {},
   "source": [
    "## 5. Funci√≥n de Evaluaci√≥n de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Eval√∫a un modelo con m√∫ltiples m√©tricas de regresi√≥n.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Valores reales\n",
    "        y_pred: Valores predichos\n",
    "        model_name: Nombre del modelo\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con las m√©tricas calculadas\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Calcular MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE (%)': mape\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úì Funci√≥n de evaluaci√≥n definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8aa4e",
   "metadata": {},
   "source": [
    "## 6. Entrenar y Evaluar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar resultados\n",
    "results = {\n",
    "    'train_metrics': [],\n",
    "    'test_metrics': [],\n",
    "    'trained_models': {},\n",
    "    'predictions': {},\n",
    "    'training_times': {}\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENTRENANDO MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Entrenando: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Medir tiempo de entrenamiento\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calcular tiempo\n",
    "    training_time = time.time() - start_time\n",
    "    results['training_times'][model_name] = training_time\n",
    "    \n",
    "    print(f\"‚úì Entrenamiento completado en {training_time:.2f} segundos\")\n",
    "    \n",
    "    # Predicciones\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Guardar predicciones\n",
    "    results['predictions'][model_name] = {\n",
    "        'train': y_train_pred,\n",
    "        'test': y_test_pred\n",
    "    }\n",
    "    \n",
    "    # Evaluar en conjunto de entrenamiento\n",
    "    train_metrics = evaluate_model(y_train, y_train_pred, model_name)\n",
    "    results['train_metrics'].append(train_metrics)\n",
    "    \n",
    "    # Evaluar en conjunto de prueba\n",
    "    test_metrics = evaluate_model(y_test, y_test_pred, model_name)\n",
    "    results['test_metrics'].append(test_metrics)\n",
    "    \n",
    "    # Guardar modelo entrenado\n",
    "    results['trained_models'][model_name] = model\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"\\nM√©tricas en TRAIN:\")\n",
    "    print(f\"  MAE:  ${train_metrics['MAE']:,.2f}\")\n",
    "    print(f\"  RMSE: ${train_metrics['RMSE']:,.2f}\")\n",
    "    print(f\"  R¬≤:   {train_metrics['R2']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nM√©tricas en TEST:\")\n",
    "    print(f\"  MAE:  ${test_metrics['MAE']:,.2f}\")\n",
    "    print(f\"  RMSE: ${test_metrics['RMSE']:,.2f}\")\n",
    "    print(f\"  R¬≤:   {test_metrics['R2']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ TODOS LOS MODELOS ENTRENADOS\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9641e18",
   "metadata": {},
   "source": [
    "## 7. Comparaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrames con los resultados\n",
    "train_results_df = pd.DataFrame(results['train_metrics'])\n",
    "test_results_df = pd.DataFrame(results['test_metrics'])\n",
    "\n",
    "# Agregar tiempos de entrenamiento\n",
    "train_results_df['Training Time (s)'] = train_results_df['Model'].map(results['training_times'])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESULTADOS EN CONJUNTO DE ENTRENAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "print(train_results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTADOS EN CONJUNTO DE PRUEBA\")\n",
    "print(\"=\" * 80)\n",
    "print(test_results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f79cea",
   "metadata": {},
   "source": [
    "### 7.1 Visualizaci√≥n de Comparaci√≥n de M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de comparaci√≥n de m√©tricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Comparaci√≥n de Modelos - M√©tricas en Test Set', fontsize=16, fontweight='bold')\n",
    "\n",
    "# MAE\n",
    "axes[0, 0].bar(test_results_df['Model'], test_results_df['MAE'], color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Mean Absolute Error (MAE)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('MAE ($)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[0, 1].bar(test_results_df['Model'], test_results_df['RMSE'], color='lightcoral', edgecolor='black')\n",
    "axes[0, 1].set_title('Root Mean Squared Error (RMSE)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('RMSE ($)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤\n",
    "axes[1, 0].bar(test_results_df['Model'], test_results_df['R2'], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('R¬≤ Score', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('R¬≤')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "# MAPE\n",
    "axes[1, 1].bar(test_results_df['Model'], test_results_df['MAPE (%)'], color='plum', edgecolor='black')\n",
    "axes[1, 1].set_title('Mean Absolute Percentage Error (MAPE)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('MAPE (%)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8ce23",
   "metadata": {},
   "source": [
    "## 8. Seleccionar el Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar mejor modelo basado en RMSE en test set\n",
    "best_model_idx = test_results_df['RMSE'].idxmin()\n",
    "best_model_name = test_results_df.loc[best_model_idx, 'Model']\n",
    "best_model = results['trained_models'][best_model_name]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MEJOR MODELO SELECCIONADO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModelo: {best_model_name}\")\n",
    "print(f\"\\nM√©tricas en Test Set:\")\n",
    "print(f\"  MAE:  ${test_results_df.loc[best_model_idx, 'MAE']:,.2f}\")\n",
    "print(f\"  RMSE: ${test_results_df.loc[best_model_idx, 'RMSE']:,.2f}\")\n",
    "print(f\"  R¬≤:   {test_results_df.loc[best_model_idx, 'R2']:.4f}\")\n",
    "print(f\"  MAPE: {test_results_df.loc[best_model_idx, 'MAPE (%)']:.2f}%\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dea270",
   "metadata": {},
   "source": [
    "## 9. Visualizaci√≥n de Predicciones del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6dc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener predicciones del mejor modelo\n",
    "y_test_pred_best = results['predictions'][best_model_name]['test']\n",
    "\n",
    "# Gr√°fico de predicciones vs valores reales\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot: Predicciones vs Valores Reales\n",
    "axes[0].scatter(y_test, y_test_pred_best, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Predicci√≥n perfecta')\n",
    "axes[0].set_xlabel('Valores Reales ($)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicciones ($)', fontsize=12)\n",
    "axes[0].set_title(f'Predicciones vs Valores Reales - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribuci√≥n de errores\n",
    "errors = y_test - y_test_pred_best\n",
    "axes[1].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Error = 0')\n",
    "axes[1].set_xlabel('Error de Predicci√≥n ($)', fontsize=12)\n",
    "axes[1].set_ylabel('Frecuencia', fontsize=12)\n",
    "axes[1].set_title(f'Distribuci√≥n de Errores - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estad√≠sticas de errores\n",
    "print(f\"\\nEstad√≠sticas de Errores ({best_model_name}):\")\n",
    "print(f\"  Media de errores: ${errors.mean():,.2f}\")\n",
    "print(f\"  Desv. Std. de errores: ${errors.std():,.2f}\")\n",
    "print(f\"  Error m√≠nimo: ${errors.min():,.2f}\")\n",
    "print(f\"  Error m√°ximo: ${errors.max():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7ab2b",
   "metadata": {},
   "source": [
    "## 10. Importancia de Features (si aplica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si el modelo tiene feature_importances_\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Obtener importancias\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Crear DataFrame con importancias\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Top 15 features m√°s importantes\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], align='center')\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Importancia', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.title(f'Top 15 Features M√°s Importantes - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Features M√°s Importantes:\")\n",
    "    print(feature_importance_df.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(f\"\\nEl modelo {best_model_name} no tiene atributo 'feature_importances_'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00550c2",
   "metadata": {},
   "source": [
    "## 11. Guardar el Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el mejor modelo\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = f\"best_model_{best_model_name.replace(' ', '_').lower()}_{timestamp}.pkl\"\n",
    "model_path = os.path.join(MODEL_OUTPUT_DIR, model_filename)\n",
    "\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELO GUARDADO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Modelo: {best_model_name}\")\n",
    "print(f\"Ruta: {model_path}\")\n",
    "print(f\"Tama√±o: {os.path.getsize(model_path) / 1024:.2f} KB\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Guardar tambi√©n los resultados de entrenamiento\n",
    "results_filename = f\"training_results_{timestamp}.pkl\"\n",
    "results_path = os.path.join(MODEL_OUTPUT_DIR, results_filename)\n",
    "\n",
    "# Preparar resultados para guardar\n",
    "results_to_save = {\n",
    "    'best_model_name': best_model_name,\n",
    "    'train_metrics': train_results_df,\n",
    "    'test_metrics': test_results_df,\n",
    "    'feature_names': feature_names,\n",
    "    'training_times': results['training_times']\n",
    "}\n",
    "\n",
    "joblib.dump(results_to_save, results_path)\n",
    "print(f\"\\n‚úì Resultados de entrenamiento guardados en: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038d4ae",
   "metadata": {},
   "source": [
    "## 12. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN DEL ENTRENAMIENTO DE MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"  ‚Ä¢ Total de muestras: {len(y_train) + len(y_test):,}\")\n",
    "print(f\"  ‚Ä¢ Muestras de entrenamiento: {len(y_train):,}\")\n",
    "print(f\"  ‚Ä¢ Muestras de prueba: {len(y_test):,}\")\n",
    "print(f\"  ‚Ä¢ Features totales: {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\nü§ñ Modelos entrenados: {len(models)}\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  ‚Ä¢ {model_name}\")\n",
    "\n",
    "print(f\"\\nüèÜ Mejor Modelo: {best_model_name}\")\n",
    "print(f\"  ‚Ä¢ RMSE: ${test_results_df.loc[best_model_idx, 'RMSE']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ R¬≤: {test_results_df.loc[best_model_idx, 'R2']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Tiempo de entrenamiento: {results['training_times'][best_model_name]:.2f}s\")\n",
    "\n",
    "print(f\"\\nüíæ Archivos guardados:\")\n",
    "print(f\"  ‚Ä¢ Modelo: {model_filename}\")\n",
    "print(f\"  ‚Ä¢ Resultados: {results_filename}\")\n",
    "\n",
    "print(f\"\\nüìà Observaciones:\")\n",
    "print(f\"  ‚Ä¢ Todos los modelos han sido entrenados exitosamente\")\n",
    "print(f\"  ‚Ä¢ El modelo con mejor desempe√±o ha sido guardado\")\n",
    "print(f\"  ‚Ä¢ Los resultados est√°n listos para evaluaci√≥n detallada\")\n",
    "\n",
    "print(f\"\\n‚úÖ Entrenamiento completado exitosamente\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
